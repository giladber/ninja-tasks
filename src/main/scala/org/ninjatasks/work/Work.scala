package org.ninjatasks.work

/**
 * A work object is a container for multiple job objects.
 * The logic and data included in this object are:
 * 1. A JobCreator object, which is responsible for the lazy initialization of
 * processable job objects.
 * 2. A combine function, which reduces the results of the underlying job objects
 * as they are processed, into a single result.
 * 3. The initial result of the computation. The final result is calculated as a reduction of the combine function
 * over the stream of incoming results, with the initial result field as its initial value.
 * 4. A unique ID.
 * 5. User-supplied priority.
 * 6. The number of underlying jobs which require processing.
 * 7. A data object, which is usually a relatively large object that is required
 * for the processing of the job objects. It is preferable that implementations of
 * this trait will include all non-job-specific data in the work object, since the
 * work object is sent to worker actors only once per *work*, while if you include
 * lots un-needed data in job objects, that data will occur a higher latency in
 * transferring the job objects themselves from the work manager to the workers,
 * and vice versa.
 *
 * The final result of this work is computed by applying the combine function
 * to the stream of incoming job results, with initialResult being the initial value
 * of the reduction
 *
 * Since the combine and creator functions will be called by actors inside the ninja-tasks framework,
 * they must absolutely be non-blocking, otherwise possibly resulting in severe scalability issues
 * for this system.
 * This requirement is especially critical for the combine function which may be called many times,
 * depending on the number of underlying job objects a work will have. If a blocking call (say network, DB-related)
 * must be made, then make sure to either make it before or after submitting the work for processing. The same applies
 * for the creator function, however that will only be called once per work submission, so it has less potential
 * to become a bottleneck.
 *
 * The priority
 *
 * @tparam T The type of results produced by processing the underlying jobs.
 * @tparam D The type of work-related data which is supplied to the job objects.
 * @tparam R The type of the final result obtained by the computation of this work.
 */
trait Work[T, D, R]
{
	/**
	 * Job objects factory, for lazy creation of jobs when required and when
	 * processing has become possible.
	 * @return A specialized instance of a JobCreator for this work object.
	 */
	def creator: JobCreator[T, D]

	/**
	 * A reduce function to compute the final result of this computation.
	 * This function will be given as input, for a finished job J with result X,
	 * (currentResult, X), and this value will be the new currentResult of this work.
	 *
	 * For example, if our work emits String result and we wished to combine them all into a list,
	 * we would supply the following combine function:
	 * (xs: List[String], x: String) => xs + x
	 *
	 * In another example, if we wanted to take the sum of all our Long results,
	 * we would supply the following combine function:
	 * (sum: Long, x: Long) => sum + x
	 *
	 * If for example we would like to take an average of all our Long results, then the combine function
	 * will need to have an inner counter of how many results it had processed so far. Assuming that counter is n,
	 * our combine function would be:
	 * (avg: Long, x: Long) => n = n + 1; (x + (n-1)*avg)/n
	 */
	val combine: (R, T) => R

	/**
	 * The initial result of the computation, which will be used as the first operand of the reduction.
	 */
	val initialResult: R

	/**
	 * A unique identifier of this work.
	 * Will be deprecated soon, in favor of unique IDs which will be generated by the framework.
	 */
	val id: Long
	//TODO deprecate ID and instead generate work ids by the framework (work mgr).

	/**
	 * Underlying data which will be used by jobs for their computation.
	 * This object should mostly consist of data that is not related to a certain job's computation,
	 * since it is sent to all processing actors, regardless of which job they are executing.
	 * On the other hand, it any data that is relevant to all jobs should be contained in this object
	 * and not in other job objects, in order to minimize serialization costs, to not incur
	 * that cost for every job that is sent.
	 */
	val data: D

	/**
	 * The priority assigned to this work.
	 * A higher value means higher priority -- that is, works with the higher priority will be processed prior to works
	 * with a lower priority, regardless of insertion time. Job objects of works with the same priority value will be sent
	 * to processing in order of insertion to the internal job queue.
	 */
	val priority: Int

	/**
	 * The total number of jobs which will be produced for processing by this work.
	 * This value is used to determine when the work's computation has finished.
	 */
	val jobNum: Long
}

/**
 * An extension to the basic work trait, with additional functional operations applicable to its underlying elements.
 * All methods in this class do not change any underlying state of the work object, instead creating a new one.
 * @tparam T The type of results produced by processing the underlying jobs.
 * @tparam D The type of work-related data which is supplied to the job objects.
 * @tparam R The type of the final result obtained by the computation of this work.
 */
trait RichWork[T, D, R] extends Work[T, D, R] with Serializable
{
	/**
	 * Returns a new work object with the given combiner, which is invoked on the results of the
	 * work's job results after the result is mapped by f.
	 * @param f mapping function
	 * @param combiner new combiner
	 * @tparam U type of the new intermediate results
	 * @return A new work object which maps the results of job objects by the mapping function.
	 */
	def mapJobResults[U](f: T => U, combiner: (R, U) => R): Work[U, D, R]

	/**
	 *
	 * @param f
	 * @tparam U
	 * @return
	 */
	def map[U](f: R => U): Work[T, D, U]

	/**
	 *
	 * @param other
	 * @tparam T2
	 * @tparam D2
	 * @tparam R2
	 * @return
	 */
	def merge[T2, D2, R2](other: Work[T2, D2, R2]): Work[Either[T, T2], Either[D, D2], (R, R2)]

	/**
	 *
	 * @param other
	 * @tparam T2
	 * @tparam D2
	 * @tparam R2
	 * @return
	 */
	def pair[T2, D2, R2](other: Work[T2, D2, R2]): Work[(T, T2), (D, D2), (R, R2)]
}

object ManagedWork {

	def apply[T, D, R](work: Work[T, D, R]): ManagedWork[T, D, R] = new ManagedWork(work)

}

/**
 * A wrapper class to the Work trait, adding the result field which is not required to be implemented by the user.
 * @param work Underlying user-supplied work object
 * @tparam T The type of results produced by processing the underlying jobs.
 * @tparam D The type of work-related data which is supplied to the job objects.
 * @tparam R The type of the final result obtained by the computation of this work.
 */
private[ninjatasks] class ManagedWork[T, D, R](val work: Work[T, D, R]) extends Work[T, D, R] with Serializable {

	require(work != null)

	override def creator = work.creator

	override val combine: (R, T) => R = work.combine

	override val initialResult: R = work.initialResult

	override val id: Long = work.id

	override val data: D = work.data

	override val priority: Int = work.priority

	override val jobNum: Long = work.jobNum

	var result: R = initialResult

	def update(additionalResult: T) =
		result = combine(result, additionalResult)

	require(creator != null)
	require(combine != null)
	require(initialResult != null)
	require(data != null)
	require(jobNum > 0)
}


